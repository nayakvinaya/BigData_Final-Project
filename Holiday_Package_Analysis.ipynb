{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-9dxATAGrmP"
      },
      "source": [
        "# <center> **M3.2 ELT and Spark SQL**</center>\n",
        "\n",
        "### <p style=\"color:purple;\"> Submission by Team Supreme : Raghuveer Karrotu , Vinaya Rajaram Nayak, Arivarasan Ramasamy, Gayathri Shanmuga Sundaram \n",
        " \n",
        "### <p style=\"color:purple;\"> Assignment Description : \n",
        "### <p style=\"color:purple;\">Load the data into a spark dataframe , Show the schema, and make any necessary changes to the data schema, Conduct any transformations, Store the data into a persistent table and Create a temp view of the data\n",
        "\n",
        "# <center> <p style=\"color:green;\"> **Holiday Package Analysis**</p>  </center>  \n",
        "#### <p style=\"color:cyan;\">Aim: To predict which customer is more likely to purchase the newly introduced travel package so that company could make its marketing expenditure more efficient.  </p> \n",
        "\n",
        "#### <p style=\"color:cyan;\">Dataset Description : The dataset we use in this analysis is from Kaggle which was obtained from “Travel.com” website. Our dataset includes various customer demographics and company features like the following \n",
        "##### CustomerID : Unique customer Id\n",
        "##### ProdTaken : This our target variable which says whether the customer purchased the product pitched.\n",
        "##### Age : Age of the customer \n",
        "##### TypeofContact : How customer was contacted (Company Invited or Self Inquiry) \n",
        "##### CityTier : City tier depends on the development of a city, population, facilities, and living standards. \n",
        "##### DurationOfPitch : Duration of the pitch by a salesperson to the customer,\n",
        "##### Occupation : Occupation of the customer\n",
        "##### Gender : Gender of the customer\n",
        "##### NumberOfPersonVisiting : Total number of persons planning to take the trip with the customer\n",
        "##### NumberOfFollowups : Total number of follow-ups has been done by the salesperson after the sales pitch\n",
        "##### ProductPitched : Product pitched by the salesperson\n",
        "##### PreferredPropertyStar : Preferred hotel property rating by customer\n",
        "##### MaritalStatus : Marital status of customer\n",
        "##### NumberOfTrips : Average number of trips in a year by customer\n",
        "##### Passport : The customer has a passport or not (0: No, 1: Yes)\n",
        "##### PitchSatisfactionScore : Sales pitch satisfaction score\n",
        "##### OwnCar : Whether the customers own a car or not (0: No, 1: Yes)\n",
        "##### NumberOfChildrenVisiting : Total number of children with age less than 5 planning to take the trip with the customer\n",
        "##### Designation : Designation of the customer in the current organization\n",
        "##### MonthlyIncome : Gross monthly income of the customer\n",
        "</p> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bed5rqEKG8b7",
        "outputId": "ac0badbc-8cf1-4f91-c944-219d77d70520"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaimwbfEGrmQ",
        "outputId": "c072c9e3-21b0-45fc-cf3a-5cc67587e420"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/05/03 01:40:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Session WebUI Port: 4040\n"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession;\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[4]\").appName(\"ISM6562 Spark Assignment App\").getOrCreate();\n",
        "\n",
        "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
        "sc = spark.sparkContext  \n",
        "\n",
        "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
        "# this spark session webUI will be on a different port than the default (4040). One way to \n",
        "# identify this part is with the following line. If there was only one spark session running, \n",
        "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
        "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
        "print(\"Spark Session WebUI Port: \" + spark_session_port)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZnUCzHriGrmR"
      },
      "outputs": [],
      "source": [
        "# this will set the log level to ERROR. This will hide the INFO or WARNING messages that are printed out by default. If you want to see them, set this to INFO or WARN.\n",
        "sc.setLogLevel(\"ERROR\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "PbVHApV6GrmS",
        "outputId": "8e4786c8-f153-48cd-d0c0-79e00d09b61a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://praveens-mbp.lan:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[4]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>ISM6562 Spark Assignment App</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe84bb093f0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OefHHjcgGrmS"
      },
      "source": [
        "## Loading our data into spark dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e9DL0mNGrmS",
        "outputId": "4d81a546-250f-40a0-9d45-914fcdd6dca2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+----+---------------+--------+---------------+--------------+------+----------------------+-----------------+--------------+---------------------+-------------+-------------+--------+----------------------+------+------------------------+--------------+-------------+\n",
            "|CustomerID|ProdTaken| Age|  TypeofContact|CityTier|DurationOfPitch|    Occupation|Gender|NumberOfPersonVisiting|NumberOfFollowups|ProductPitched|PreferredPropertyStar|MaritalStatus|NumberOfTrips|Passport|PitchSatisfactionScore|OwnCar|NumberOfChildrenVisiting|   Designation|MonthlyIncome|\n",
            "+----------+---------+----+---------------+--------+---------------+--------------+------+----------------------+-----------------+--------------+---------------------+-------------+-------------+--------+----------------------+------+------------------------+--------------+-------------+\n",
            "|    200000|        1|  41|   Self Enquiry|       3|              6|      Salaried|Female|                     3|                3|        Deluxe|                    3|       Single|            1|       1|                     2|     1|                       0|       Manager|        20993|\n",
            "|    200001|        0|  49|Company Invited|       1|             14|      Salaried|  Male|                     3|                4|        Deluxe|                    4|     Divorced|            2|       0|                     3|     1|                       2|       Manager|        20130|\n",
            "|    200002|        1|  37|   Self Enquiry|       1|              8|   Free Lancer|  Male|                     3|                4|         Basic|                    3|       Single|            7|       1|                     3|     0|                       0|     Executive|        17090|\n",
            "|    200003|        0|  33|Company Invited|       1|              9|      Salaried|Female|                     2|                3|         Basic|                    3|     Divorced|            2|       1|                     5|     1|                       1|     Executive|        17909|\n",
            "|    200004|        0|null|   Self Enquiry|       1|              8|Small Business|  Male|                     2|                3|         Basic|                    4|     Divorced|            1|       0|                     5|     1|                       0|     Executive|        18468|\n",
            "|    200005|        0|  32|Company Invited|       1|              8|      Salaried|  Male|                     3|                3|         Basic|                    3|       Single|            1|       0|                     5|     1|                       1|     Executive|        18068|\n",
            "|    200006|        0|  59|   Self Enquiry|       1|              9|Small Business|Female|                     2|                2|         Basic|                    5|     Divorced|            5|       1|                     2|     1|                       1|     Executive|        17670|\n",
            "|    200007|        0|  30|   Self Enquiry|       1|             30|      Salaried|  Male|                     3|                3|         Basic|                    3|      Married|            2|       0|                     2|     0|                       1|     Executive|        17693|\n",
            "|    200008|        0|  38|Company Invited|       1|             29|      Salaried|  Male|                     2|                4|      Standard|                    3|    Unmarried|            1|       0|                     3|     0|                       0|Senior Manager|        24526|\n",
            "|    200009|        0|  36|   Self Enquiry|       1|             33|Small Business|  Male|                     3|                3|        Deluxe|                    3|     Divorced|            7|       0|                     3|     1|                       0|       Manager|        20237|\n",
            "|    200010|        0|  35|   Self Enquiry|       1|             22|Small Business|  Male|                     2|                2|         Basic|                    4|     Divorced|            1|       0|                     3|     1|                       1|     Executive|        17426|\n",
            "|    200011|        0|null|   Self Enquiry|       1|             21|      Salaried|Female|                     2|                4|        Deluxe|                    3|       Single|            1|       1|                     3|     0|                       0|       Manager|         null|\n",
            "|    200012|        0|  31|   Self Enquiry|       1|             32|      Salaried|  Male|                     2|                3|         Basic|                    3|      Married|            2|       0|                     3|     0|                       1|     Executive|        17911|\n",
            "|    200013|        0|  34|   Self Enquiry|       1|             25|Small Business|  Male|                     3|                3|         Basic|                    3|      Married|            1|       0|                     3|     0|                       2|     Executive|        17661|\n",
            "|    200014|        1|  28|   Self Enquiry|       1|             30|      Salaried|  Male|                     2|                4|         Basic|                    3|       Single|            6|       1|                     2|     0|                       0|     Executive|        17028|\n",
            "|    200015|        0|  29|   Self Enquiry|       1|             27|      Salaried|Female|                     2|                2|      Standard|                    5|      Married|            2|       0|                     5|     1|                       1|Senior Manager|        24980|\n",
            "|    200016|        0|  32|   Self Enquiry|       1|             11|      Salaried|  Male|                     3|                2|         Basic|                    4|      Married|            1|       1|                     2|     1|                       0|     Executive|        18298|\n",
            "|    200017|        0|  22|Company Invited|       1|             22|Small Business|  Male|                     3|                2|         Basic|                    3|      Married|            2|       1|                     3|     0|                       0|     Executive|        17935|\n",
            "|    200018|        0|  53|   Self Enquiry|       3|              8|      Salaried|Female|                     3|                4|  Super Deluxe|                    3|     Divorced|            3|       0|                     3|     1|                       0|           AVP|        30427|\n",
            "|    200019|        0|null|   Self Enquiry|       1|              8|      Salaried|  Male|                     2|                3|         Basic|                    3|       Single|            6|       1|                     4|     0|                       1|     Executive|         null|\n",
            "+----------+---------+----+---------------+--------+---------------+--------------+------+----------------------+-----------------+--------------+---------------------+-------------+-------------+--------+----------------------+------+------------------------+--------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load CSV file\n",
        "df_spark = spark.read.csv(\"Travel.csv\", header=True, inferSchema=True)\n",
        "df_spark.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ3GVjjLGrmS"
      },
      "source": [
        "# Data Exploration and transformations "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrKARWknGrmS",
        "outputId": "a6b0b37d-4a72-4d99-8b42-6909a224701c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- ProdTaken: integer (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- TypeofContact: string (nullable = true)\n",
            " |-- CityTier: integer (nullable = true)\n",
            " |-- DurationOfPitch: integer (nullable = true)\n",
            " |-- Occupation: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- NumberOfPersonVisiting: integer (nullable = true)\n",
            " |-- NumberOfFollowups: integer (nullable = true)\n",
            " |-- ProductPitched: string (nullable = true)\n",
            " |-- PreferredPropertyStar: integer (nullable = true)\n",
            " |-- MaritalStatus: string (nullable = true)\n",
            " |-- NumberOfTrips: integer (nullable = true)\n",
            " |-- Passport: integer (nullable = true)\n",
            " |-- PitchSatisfactionScore: integer (nullable = true)\n",
            " |-- OwnCar: integer (nullable = true)\n",
            " |-- NumberOfChildrenVisiting: integer (nullable = true)\n",
            " |-- Designation: string (nullable = true)\n",
            " |-- MonthlyIncome: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_spark.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDTFAEBMGrmS"
      },
      "source": [
        "# Visualizing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "yUkEZ0ZcGrmS",
        "outputId": "6bafac92-9cbe-465c-a075-b68d35475aef"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df_spark is the Spark DataFrame containing the column of interest\n",
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "df_pandas = df_spark.toPandas()\n",
        "\n",
        "sns.countplot(x='DurationOfPitch', data=df_pandas) # checking distribution of \"durationofpitch\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaEg57-KGrmT"
      },
      "source": [
        "# Finding missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-ebGmgzGrmT",
        "outputId": "c2cee51f-b1fa-4fd3-ad65-ade585e369a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns with missing values:\n",
            "Age\n",
            "TypeofContact\n",
            "DurationOfPitch\n",
            "NumberOfFollowups\n",
            "PreferredPropertyStar\n",
            "NumberOfTrips\n",
            "NumberOfChildrenVisiting\n",
            "MonthlyIncome\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Find columns with missing values\n",
        "columns_with_missing_values = [column for column in df_spark.columns if df_spark.filter(col(column).isNull()).count() > 0]\n",
        "\n",
        "# Print columns with missing values\n",
        "print(\"Columns with missing values:\")\n",
        "for column in columns_with_missing_values:\n",
        "    print(column)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaMUtWT3GrmT",
        "outputId": "828e08f4-5ea5-434f-8aa0-97614770b2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----+\n",
            "|  TypeofContact|count|\n",
            "+---------------+-----+\n",
            "|           null|   25|\n",
            "|   Self Enquiry| 3444|\n",
            "|Company Invited| 1419|\n",
            "+---------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Group by the column and apply the count() function\n",
        "count_df = df_spark.groupBy(\"TypeofContact\").count()\n",
        "\n",
        "# Show the resulting counts\n",
        "count_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fUAgJ8KGrmT"
      },
      "source": [
        "# Imputing missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_nHTA9gGrmT"
      },
      "source": [
        "Since, TypeofContact column is in string object, imputing with the most frequent value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t7eB-im6GrmT"
      },
      "outputs": [],
      "source": [
        "df_spark = df_spark.fillna(\"Self Enquiry\", subset=[\"TypeofContact\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GAMtu-4GrmU"
      },
      "source": [
        "Imputing with missing value with median value for numeric values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UoQjtY4sGrmU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import percentile_approx\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Iterate over columns with missing values\n",
        "for column in columns_with_missing_values:\n",
        "    # Calculate median of the column\n",
        "    median_value = df_spark.select(column).agg(percentile_approx(column, 0.5)).collect()[0][0]\n",
        "    if median_value is not None:\n",
        "    # Round median_value to nearest integer\n",
        "        median_value_rounded = int(round(median_value))\n",
        "    else:\n",
        "        median_value_rounded = 0\n",
        "    \n",
        "    # Impute missing values with median value\n",
        "    df_spark = df_spark.withColumn(column, when(col(column).isNull(), median_value_rounded).otherwise(col(column)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVLl0_GYGrmU",
        "outputId": "3090970b-a1a8-45bc-a2d2-1128b78d7e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns with missing values:\n"
          ]
        }
      ],
      "source": [
        "# verifying if all missing values were imputed\n",
        "columns_with_missing_values = [column for column in df_spark.columns if df_spark.filter(col(column).isNull()).count() > 0]\n",
        "\n",
        "# Print columns with missing values\n",
        "print(\"Columns with missing values:\")\n",
        "for column in columns_with_missing_values:\n",
        "    print(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9pIocpppGrmU"
      },
      "outputs": [],
      "source": [
        "# Export DataFrame to CSV\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "output_folder = \"processedfolder\"\n",
        "\n",
        "# check if the output folder already exists\n",
        "if os.path.isdir(output_folder):\n",
        "    # remove the folder if it already exists\n",
        "    shutil.rmtree(output_folder)\n",
        "\n",
        "df_spark.write.csv(output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJiICWsqGrmU",
        "outputId": "a96782e1-2d20-466f-fe27-c9744754bd4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+---+---------------+--------+---------------+-----------+------+----------------------+-----------------+--------------+---------------------+-------------+-------------+--------+----------------------+------+------------------------+-----------+-------------+\n",
            "|CustomerID|ProdTaken|Age|  TypeofContact|CityTier|DurationOfPitch| Occupation|Gender|NumberOfPersonVisiting|NumberOfFollowups|ProductPitched|PreferredPropertyStar|MaritalStatus|NumberOfTrips|Passport|PitchSatisfactionScore|OwnCar|NumberOfChildrenVisiting|Designation|MonthlyIncome|\n",
            "+----------+---------+---+---------------+--------+---------------+-----------+------+----------------------+-----------------+--------------+---------------------+-------------+-------------+--------+----------------------+------+------------------------+-----------+-------------+\n",
            "|    200000|        1| 41|   Self Enquiry|       3|              6|   Salaried|Female|                     3|                3|        Deluxe|                    3|       Single|            1|       1|                     2|     1|                       0|    Manager|        20993|\n",
            "|    200001|        0| 49|Company Invited|       1|             14|   Salaried|  Male|                     3|                4|        Deluxe|                    4|     Divorced|            2|       0|                     3|     1|                       2|    Manager|        20130|\n",
            "|    200002|        1| 37|   Self Enquiry|       1|              8|Free Lancer|  Male|                     3|                4|         Basic|                    3|       Single|            7|       1|                     3|     0|                       0|  Executive|        17090|\n",
            "+----------+---------+---+---------------+--------+---------------+-----------+------+----------------------+-----------------+--------------+---------------------+-------------+-------------+--------+----------------------+------+------------------------+-----------+-------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_spark.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3pEqYQ0GrmU"
      },
      "source": [
        "# Store the data into a persistent table and create a temp view of the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THnyLlvOGrmU",
        "outputId": "31e8683b-d8fe-4312-c316-98129e4b9b44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_folder = \"spark-warehouse/travel.db\"\n",
        "\n",
        "# check if the output folder already exists\n",
        "if os.path.isdir(output_folder):\n",
        "    # remove the folder if it already exists\n",
        "    shutil.rmtree(output_folder)\n",
        "\n",
        "df_spark.write.csv(output_folder)\n",
        "\n",
        "\n",
        "# Create a database\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS Travel\")\n",
        "\n",
        "# Use the database\n",
        "spark.sql(\"USE Travel\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ks_QYIKFGrmU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Store the data into a persistent table in the Travel db\n",
        "#df_spark.write.mode(\"ignore\").saveAsTable(\"travel_information\")\n",
        "\n",
        "spark.sql(\"DROP TABLE IF EXISTS travel_information\")\n",
        "\n",
        "df_spark.write.saveAsTable(\"travel_information\", mode='overwrite')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oBLn95C_GrmU"
      },
      "outputs": [],
      "source": [
        "# Create a temporary view of the data\n",
        "df_spark.createOrReplaceTempView(\"travel_information_view\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3hDXUN4GrmU",
        "outputId": "0850621e-9c3a-4eed-e456-e3cc21535661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table exists in the created database.\n"
          ]
        }
      ],
      "source": [
        "# Verify if the table exists in the created database\n",
        "check = spark.sql(\"SHOW TABLES\")\n",
        "if check.filter(check.tableName == \"travel_information_view\").count() > 0:\n",
        "    print(\"Table exists in the created database.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKf5vK-JGrmV"
      },
      "source": [
        "# Now let us find some insights by using the aggregation.\n",
        "#### <p style=\"color:orange;\"> 1. The average age of customers </p>\n",
        "<p style=\"color:orange;\">First we are interested to know the average age of customers </p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ymj_1NdGrmV",
        "outputId": "005481de-e2c3-4a8b-b76e-207640749f29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|          avg_age|\n",
            "+-----------------+\n",
            "|37.54725859247136|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "avg_age_result = spark.sql(\"SELECT AVG(Age) as avg_age FROM travel_information_view\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaKMcwT_GrmV"
      },
      "source": [
        "#### <p style=\"color:orange;\"> 2. Occupation wise display </p>\n",
        "<p style=\"color:orange;\">Next let us calculate  the number of customers who have taken a product in  each occupation. We calculate the total number of customers and no of customers who bought the product in each occupation.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5KMzzLOGrmV",
        "outputId": "f92c956f-06b3-495c-c51f-689548c14d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+---------------+-----------------------+\n",
            "|    Occupation|total_customers|customers_taken_product|\n",
            "+--------------+---------------+-----------------------+\n",
            "|      Salaried|           2368|                    414|\n",
            "|Large Business|            434|                    120|\n",
            "|   Free Lancer|              2|                      2|\n",
            "|Small Business|           2084|                    384|\n",
            "+--------------+---------------+-----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "total_result = spark.sql(\"\"\"\n",
        "    SELECT Occupation, COUNT(*) AS total_customers, SUM(ProdTaken) AS customers_taken_product\n",
        "    FROM travel_information_view\n",
        "    GROUP BY Occupation\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcRk1Ji-GrmV"
      },
      "source": [
        "#### <p style=\"color:orange;\"> 3. Occupation wise average age and monthly income  </p>\n",
        "<p style=\"color:orange;\">Now let us calculate the average age and monthly income for each occupation. We have grouped the avg age and monthlyincome by occupation for the below results </p> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We9cdAQZGrmV",
        "outputId": "b3c191b7-08cf-4afd-ff40-8f2b02d91a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+------------------+------------------+\n",
            "|    Occupation|           avg_age|avg_monthly_income|\n",
            "+--------------+------------------+------------------+\n",
            "|      Salaried|37.569679054054056| 23591.93918918919|\n",
            "|Large Business| 36.60829493087557|22859.873271889403|\n",
            "|   Free Lancer|              37.5|           18929.0|\n",
            "|Small Business| 37.71737044145873|23672.031669865642|\n",
            "+--------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ocupation_results = spark.sql(\"\"\"WITH cte AS (\n",
        "    SELECT Occupation, AVG(Age) AS avg_age, AVG(`MonthlyIncome`) AS avg_monthly_income\n",
        "    FROM travel_information_view\n",
        "    GROUP BY Occupation\n",
        ")\n",
        "SELECT Occupation, avg_age, avg_monthly_income\n",
        "FROM cte\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqA_zNLgGrmV"
      },
      "source": [
        "#### <p style=\"color:orange;\"> 4. Percentage of customers who bought the package pitched. </p>\n",
        "<p style=\"color:orange;\">Finally let us calculate the the percentage of customers who have taken a product based on the product pitched. For this we selected the product pitched, count of customers and sum of the prodtaken and then grouped by product pitched to get the necessary results </p>  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntAMisnUGrmV",
        "outputId": "5118c3d9-5446-48a6-b811-60cc838bf4da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+---------------+-----------------------+------------------------+\n",
            "|ProductPitched|total_customers|customers_taken_product|percentage_taken_product|\n",
            "+--------------+---------------+-----------------------+------------------------+\n",
            "|         Basic|           1842|                    552|      29.967426710097723|\n",
            "|      Standard|            742|                    124|      16.711590296495956|\n",
            "|        Deluxe|           1732|                    204|      11.778290993071593|\n",
            "|          King|            230|                     20|       8.695652173913043|\n",
            "|  Super Deluxe|            342|                     20|       5.847953216374268|\n",
            "+--------------+---------------+-----------------------+------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "purchased_percent_result = spark.sql(\"\"\"\n",
        "    SELECT ProductPitched, COUNT(*) AS total_customers, SUM(ProdTaken) AS customers_taken_product,\n",
        "           (SUM(ProdTaken) / COUNT(*) * 100) AS percentage_taken_product\n",
        "    FROM travel_information_view\n",
        "    GROUP BY ProductPitched\n",
        "    ORDER BY percentage_taken_product DESC\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5USVPV5XGrmV"
      },
      "source": [
        "## Check for data imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC9SwYz2GrmV",
        "outputId": "25318e61-ecb7-43a9-d5e1-ad22717e93ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|ProdTaken|count|\n",
            "+---------+-----+\n",
            "|        1|  920|\n",
            "|        0| 3968|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''ProdTaken_result = spark.sql(\"\"\"\n",
        "    SELECT ProdTaken, COUNT(*) AS total_customers\n",
        "    FROM travel_information_view\n",
        "    GROUP BY ProdTaken\n",
        "\"\"\").show()'''\n",
        "\n",
        "prodTakenCounts = df_spark.groupBy(\"ProdTaken\").count()\n",
        "prodTakenCounts.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A_oG0NWGrmV"
      },
      "source": [
        "We can see that customer who taken Product count(920) is less than and customers who did not taken the product. This is not balanced dataset. If we train any ML model with this unbalanced dataset, it will give a biased result towwards the major category. To fix this, we are trying to oversample the minority category data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLbB6tL8GrmV"
      },
      "source": [
        "## Over sampling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MXB5T2okGrmV"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import rand\n",
        "\n",
        "ProdTakenCount = prodTakenCounts.orderBy(col(\"count\").asc()).first()[1]\n",
        "\n",
        "ProdnotTakenCount = prodTakenCounts.orderBy(col(\"count\").desc()).first()[1]\n",
        "\n",
        "# Calculate the desired number of samples for the minority class\n",
        "diffrence_count = ProdnotTakenCount - ProdTakenCount\n",
        "\n",
        "oversampleRatio = round(float(diffrence_count) / float(ProdTakenCount), 2)\n",
        " \n",
        "Prodtaken_df = df_spark.filter(df_spark.ProdTaken == 1)\n",
        "\n",
        "# Select  random rows from Prodtaken_df \n",
        "sample_df = Prodtaken_df.sample(withReplacement = True, fraction=oversampleRatio) # it generated little extra sample than necessary due to fraction input value\n",
        "sample_df = sample_df.orderBy(rand()).limit(diffrence_count) # selecting exact count sample to balance the diffrence \n",
        "\n",
        "df_spark = df_spark.union(sample_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c32WDnjtGrmW",
        "outputId": "f0a9d6b5-04eb-4221-c34a-19cfc5c07ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|ProdTaken|count|\n",
            "+---------+-----+\n",
            "|        1| 3968|\n",
            "|        0| 3968|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# verifying if the data is balanced \n",
        "\n",
        "prodTakenCounts = df_spark.groupBy(\"ProdTaken\").count()\n",
        "\n",
        "prodTakenCounts.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Metric of Evaluation - Recall\n",
        "\n",
        "This is a binary class classification problem with target column- \"ProdTaken\"- (0/1)\n",
        "\n",
        "False Negatives- The model predicts that the customer does not buy the product but in reality, the customer purchases the product.\n",
        "\n",
        "False Positives-  The model predicts that the customer purchases the product but in reality, he/she does not.\n",
        "\n",
        "In this problem, the revenue brought in by a customer purchasing the product is much more than the cost of marketing. So, we cannot afford to lose a potential customer and ready to trade-off some marketing cost on non-potential customers.  \n",
        "In other words, we cannot afford False Negatives. \n",
        "\n",
        "\n",
        "Since, recall is sensitive to False Negatives,\"Recall\" is the metric of evaluation for this problem. \n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "q_BRISHuGrmW"
      },
      "outputs": [],
      "source": [
        "data=df_spark.select([\n",
        "    'Age',\n",
        "    'TypeofContact',\n",
        "    'CityTier',\n",
        "    'ProdTaken',\n",
        "    'DurationOfPitch',\n",
        "    'Occupation',\n",
        "    'Gender',\n",
        "    'ProductPitched',\n",
        "    'NumberOfFollowups',\n",
        "    'MaritalStatus',\n",
        "    'PitchSatisfactionScore',\n",
        "    'OwnCar',\n",
        "    'Designation',\n",
        "    'NumberOfChildrenVisiting',\n",
        "    'MonthlyIncome'\n",
        "    ]\n",
        ")\n",
        "data=data.dropna()\n",
        "\n",
        "# picking relevant columns for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "do3djIPDGrmW"
      },
      "outputs": [],
      "source": [
        "train_data,test_data=data.randomSplit([0.7,0.3])\n",
        "#splitting train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az9weRozGrmW",
        "outputId": "65269cd8-8aed-4e96-8265-ac2cd160724c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- TypeofContact: string (nullable = false)\n",
            " |-- CityTier: integer (nullable = true)\n",
            " |-- ProdTaken: integer (nullable = true)\n",
            " |-- DurationOfPitch: integer (nullable = true)\n",
            " |-- Occupation: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- ProductPitched: string (nullable = true)\n",
            " |-- NumberOfFollowups: integer (nullable = true)\n",
            " |-- MaritalStatus: string (nullable = true)\n",
            " |-- PitchSatisfactionScore: integer (nullable = true)\n",
            " |-- OwnCar: integer (nullable = true)\n",
            " |-- Designation: string (nullable = true)\n",
            " |-- NumberOfChildrenVisiting: integer (nullable = true)\n",
            " |-- MonthlyIncome: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6Jy5RvK8GrmW"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler,StringIndexer,StandardScaler ,OneHotEncoder\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fTZilAz6GrmW"
      },
      "outputs": [],
      "source": [
        "TypeofContact_indexer = StringIndexer(inputCol='TypeofContact',outputCol='TypeofContact_index',handleInvalid='keep')\n",
        "Occupation_indexer = StringIndexer(inputCol='Occupation',outputCol='Occupation_index',handleInvalid='keep')\n",
        "Gender_indexer = StringIndexer(inputCol='Gender',outputCol='Gender_index',handleInvalid='keep')\n",
        "ProductPitched_indexer = StringIndexer(inputCol='ProductPitched',outputCol='ProductPitched_index',handleInvalid='keep')\n",
        "MaritalStatus_indexer = StringIndexer(inputCol='MaritalStatus',outputCol='MaritalStatus_index',handleInvalid='keep')\n",
        "Designation_indexer = StringIndexer(inputCol='Designation',outputCol='Designation_index',handleInvalid='keep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "encoder = OneHotEncoder(\n",
        "    inputCols=[\n",
        "        'TypeofContact_index',\n",
        "        'Occupation_index',\n",
        "        'Gender_index',\n",
        "        'ProductPitched_index',\n",
        "        'MaritalStatus_index',\n",
        "        'Designation_index',\n",
        "    ], \n",
        "    outputCols= [\n",
        "        'TypeofContact_vec',\n",
        "        'Occupation_vec',\n",
        "        'Gender_vec',\n",
        "        'ProductPitched_vec',\n",
        "        'MaritalStatus_vec',\n",
        "        'Designation_vec'],\n",
        "    handleInvalid='keep'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xGp3eQoeGrmW"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols=[\n",
        "        'Age',\n",
        "        'TypeofContact_vec',\n",
        "        'CityTier',\n",
        "        'Occupation_vec',\n",
        "        'Gender_vec',\n",
        "        'ProductPitched_vec',\n",
        "        'NumberOfFollowups',\n",
        "        'MaritalStatus_vec',\n",
        "        'Designation_vec',\n",
        "        'DurationOfPitch',\n",
        "        'PitchSatisfactionScore',\n",
        "        'OwnCar',\n",
        "        'NumberOfChildrenVisiting',\n",
        "        'MonthlyIncome'\n",
        "    ],\n",
        "    outputCol=\"features\"\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "okjcLOjrNHiJ"
      },
      "source": [
        "## Model 1 - Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "c1C3eL_1NHiJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lreg_model = LogisticRegression(labelCol='ProdTaken')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4tu8eMgGby-e"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Pipeline is used to pass the data through indexer and assembler simultaneously. Also, it helps to pre-rocess the test data\n",
        "# in the same way as that of the train data\n",
        "# https://spark.apache.org/docs/latest/ml-pipeline.html\n",
        " \n",
        "pipe = Pipeline(stages=[\n",
        "    TypeofContact_indexer,\n",
        "    Occupation_indexer,\n",
        "    Gender_indexer,\n",
        "    ProductPitched_indexer,\n",
        "    MaritalStatus_indexer,\n",
        "    Designation_indexer,\n",
        "    encoder,\n",
        "    assembler,\n",
        "    lreg_model,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "A2llT4E4qjji",
        "outputId": "fb8c75a6-b1e7-48dc-e8f4-4c8786739db6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# run the pipeline\n",
        "fit_model=pipe.fit(train_data)\n",
        "\n",
        "# Store the results in a dataframe\n",
        "result = fit_model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlZjxVmfsO_s",
        "outputId": "6133657f-aff5-419e-e20e-8b92cfdbd19a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+\n",
            "|ProdTaken|prediction|\n",
            "+---------+----------+\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        0|       1.0|\n",
            "|        0|       1.0|\n",
            "|        0|       1.0|\n",
            "|        0|       1.0|\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        0|       1.0|\n",
            "|        0|       1.0|\n",
            "|        0|       1.0|\n",
            "|        0|       1.0|\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        0|       1.0|\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        0|       1.0|\n",
            "+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(['ProdTaken','prediction']).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdCj3ifmMMxB"
      },
      "source": [
        "#### Model evaluation- Logistic regression"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QYfywFbSsYbP"
      },
      "source": [
        "##### Area under the ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dOzpjDBOsaS2"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='ProdTaken',metricName='areaUnderROC')\n",
        "\n",
        "AUC = AUC_evaluator.evaluate(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh7YHudksX5u",
        "outputId": "c0f1ef1e-536b-4167-8f72-c6d79464beab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The area under the curve is 0.6867103109656301\n"
          ]
        }
      ],
      "source": [
        "print(\"The area under the curve is {}\".format(AUC))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E9ggptJZsi7j"
      },
      "source": [
        "##### Area under the PR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MTflXoJDsktL"
      },
      "outputs": [],
      "source": [
        "PR_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='ProdTaken',metricName='areaUnderPR')\n",
        "PR = PR_evaluator.evaluate(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_JJ3kWtBsdv1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The area under the PR curve is 0.6485460292927681\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"The area under the PR curve is {}\".format(PR))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gJsgPJrasmo3"
      },
      "source": [
        "##### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "skTCG_OMsqCE"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "ACC_evaluator = MulticlassClassificationEvaluator(  #  Multiclass or Binary, the accuracy is calculated in the same way.\n",
        "    labelCol=\"ProdTaken\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "accuracy = ACC_evaluator.evaluate(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ7yLrl5srvz",
        "outputId": "4472c20b-0eb1-46e6-a070-97e5cad4d394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy of the model is 0.686691697955778\n"
          ]
        }
      ],
      "source": [
        "print(\"The accuracy of the model is {}\".format(accuracy))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xITIRjwAsuoQ"
      },
      "source": [
        "##### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Y8cOCKBWswgP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Vb8ug_Axsx5-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is the confusion matrix \n",
            " [[838 384]\n",
            " [367 808]]\n"
          ]
        }
      ],
      "source": [
        "y_true = result.select(\"ProdTaken\")\n",
        "y_true = y_true.toPandas()\n",
        " \n",
        "y_pred = result.select(\"prediction\")\n",
        "y_pred = y_pred.toPandas()\n",
        " \n",
        "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"Below is the confusion matrix \\n {}\".format(cnf_matrix))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBYJ1jp2sziJ",
        "outputId": "89d10464-ea43-40cc-e215-46b06dc65509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is the confusion matrix \n",
            " [[838 384]\n",
            " [367 808]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Below is the confusion matrix \\n {}\".format(cnf_matrix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVHPPzIhs0_3",
        "outputId": "ad13f595-8051-4e6f-e9cb-48e1ea930868"
      },
      "outputs": [],
      "source": [
        "tn = cnf_matrix[0][0]\n",
        "fp = cnf_matrix[0][1]\n",
        "fn = cnf_matrix[1][0]\n",
        "tp = cnf_matrix[1][1]\n",
        "\n",
        "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "f1_score = 2*(precision*recall)/(precision+recall)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsV5B3Aas2Yl",
        "outputId": "b2b51c22-7b52-4ce9-a238-16f7e2511775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.69\n",
            "Precision: 0.68\n",
            "Recall: 0.69\n",
            "F1 Score: 0.68\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1_score:.2f}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model-2 Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt_model = DecisionTreeClassifier(labelCol='ProdTaken',maxBins=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_dec = Pipeline(stages=[\n",
        "    TypeofContact_indexer,\n",
        "    Occupation_indexer,\n",
        "    Gender_indexer,\n",
        "    ProductPitched_indexer,\n",
        "    MaritalStatus_indexer,\n",
        "    Designation_indexer,\n",
        "    encoder,\n",
        "    assembler,\n",
        "    dt_model\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "fit_model_dec=pipe_dec.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------------+--------+---------+---------------+--------------+------+--------------+-----------------+-------------+----------------------+------+-----------+------------------------+-------------+-------------------+----------------+------------+--------------------+-------------------+-----------------+-----------------+--------------+-------------+------------------+-----------------+---------------+--------------------+-------------+--------------------+----------+\n",
            "|Age|  TypeofContact|CityTier|ProdTaken|DurationOfPitch|    Occupation|Gender|ProductPitched|NumberOfFollowups|MaritalStatus|PitchSatisfactionScore|OwnCar|Designation|NumberOfChildrenVisiting|MonthlyIncome|TypeofContact_index|Occupation_index|Gender_index|ProductPitched_index|MaritalStatus_index|Designation_index|TypeofContact_vec|Occupation_vec|   Gender_vec|ProductPitched_vec|MaritalStatus_vec|Designation_vec|            features|rawPrediction|         probability|prediction|\n",
            "+---+---------------+--------+---------+---------------+--------------+------+--------------+-----------------+-------------+----------------------+------+-----------+------------------------+-------------+-------------------+----------------+------------+--------------------+-------------------+-----------------+-----------------+--------------+-------------+------------------+-----------------+---------------+--------------------+-------------+--------------------+----------+\n",
            "| 18|Company Invited|       1|        1|             14|Small Business|  Male|         Basic|                3|       Single|                     5|     1|  Executive|                       0|        16904|                1.0|             1.0|         0.0|                 0.0|                1.0|              0.0|    (3,[1],[1.0])| (5,[1],[1.0])|(4,[0],[1.0])|     (6,[0],[1.0])|    (5,[1],[1.0])|  (6,[0],[1.0])|(37,[0,2,4,6,10,1...|[100.0,478.0]|[0.17301038062283...|       1.0|\n",
            "| 18|   Self Enquiry|       1|        1|              9|      Salaried|  Male|         Basic|                3|       Single|                     4|     0|  Executive|                       0|        16420|                0.0|             0.0|         0.0|                 0.0|                1.0|              0.0|    (3,[0],[1.0])| (5,[0],[1.0])|(4,[0],[1.0])|     (6,[0],[1.0])|    (5,[1],[1.0])|  (6,[0],[1.0])|(37,[0,1,4,5,10,1...|[100.0,478.0]|[0.17301038062283...|       1.0|\n",
            "| 18|   Self Enquiry|       1|        1|              9|Small Business|  Male|         Basic|                3|       Single|                     4|     1|  Executive|                       0|        16420|                0.0|             1.0|         0.0|                 0.0|                1.0|              0.0|    (3,[0],[1.0])| (5,[1],[1.0])|(4,[0],[1.0])|     (6,[0],[1.0])|    (5,[1],[1.0])|  (6,[0],[1.0])|(37,[0,1,4,6,10,1...|[100.0,478.0]|[0.17301038062283...|       1.0|\n",
            "+---+---------------+--------+---------+---------------+--------------+------+--------------+-----------------+-------------+----------------------+------+-----------+------------------------+-------------+-------------------+----------------+------------+--------------------+-------------------+-----------------+-----------------+--------------+-------------+------------------+-----------------+---------------+--------------------+-------------+--------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results_dec = fit_model_dec.transform(test_data)\n",
        "results_dec.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+\n",
            "|ProdTaken|prediction|\n",
            "+---------+----------+\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        0|       1.0|\n",
            "|        0|       1.0|\n",
            "|        0|       1.0|\n",
            "|        0|       1.0|\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "+---------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results_dec.select(['ProdTaken','prediction']).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy of the decision tree classifier is 0.7125573633708803\n"
          ]
        }
      ],
      "source": [
        "ACC_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"ProdTaken\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "accuracy = ACC_evaluator.evaluate(results_dec)\n",
        "\n",
        "print(f\"The accuracy of the decision tree classifier is {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The area under the curve is 0.7113093289689034\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='ProdTaken',metricName='areaUnderROC')\n",
        "\n",
        "AUC = AUC_evaluator.evaluate(results_dec)\n",
        "print(\"The area under the curve is {}\".format(AUC))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The area under the PR curve is 0.6915070479025424\n"
          ]
        }
      ],
      "source": [
        "PR_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='ProdTaken',metricName='areaUnderPR')\n",
        "PR = PR_evaluator.evaluate(results_dec)\n",
        "print(\"The area under the PR curve is {}\".format(PR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is the confusion matrix \n",
            " [[947 275]\n",
            " [414 761]]\n"
          ]
        }
      ],
      "source": [
        "y_true_dec = results_dec.select(\"ProdTaken\")\n",
        "y_true_dec = y_true_dec.toPandas()\n",
        " \n",
        "y_pred_dec = results_dec.select(\"prediction\")\n",
        "y_pred_dec = y_pred_dec.toPandas()\n",
        " \n",
        "cnf_matrix_dec = confusion_matrix(y_true_dec, y_pred_dec)\n",
        "\n",
        "print(\"Below is the confusion matrix \\n {}\".format(cnf_matrix_dec))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.71\n",
            "Precision: 0.73\n",
            "Recall: 0.65\n",
            "F1 Score: 0.69\n"
          ]
        }
      ],
      "source": [
        "tn = cnf_matrix_dec[0][0]\n",
        "fp = cnf_matrix_dec[0][1]\n",
        "fn = cnf_matrix_dec[1][0]\n",
        "tp = cnf_matrix_dec[1][1]\n",
        "\n",
        "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "f1_score = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1_score:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model-3 Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scaling to normalize the data input to svm\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features\",outputCol=\"scaled_features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_model = LinearSVC(labelCol='ProdTaken',featuresCol=\"scaled_features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_svm = Pipeline(stages=[\n",
        "    TypeofContact_indexer,\n",
        "    Occupation_indexer,\n",
        "    Gender_indexer,\n",
        "    ProductPitched_indexer,\n",
        "    MaritalStatus_indexer,\n",
        "    Designation_indexer,\n",
        "    encoder,\n",
        "    assembler,\n",
        "    scaler,\n",
        "    svm_model,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "fit_model=pipe_svm.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[Age: int, TypeofContact: string, CityTier: int, ProdTaken: int, DurationOfPitch: int, Occupation: string, Gender: string, ProductPitched: string, NumberOfFollowups: int, MaritalStatus: string, PitchSatisfactionScore: int, OwnCar: int, Designation: string, NumberOfChildrenVisiting: int, MonthlyIncome: int, TypeofContact_index: double, Occupation_index: double, Gender_index: double, ProductPitched_index: double, MaritalStatus_index: double, Designation_index: double, TypeofContact_vec: vector, Occupation_vec: vector, Gender_vec: vector, ProductPitched_vec: vector, MaritalStatus_vec: vector, Designation_vec: vector, features: vector, scaled_features: vector, rawPrediction: vector, prediction: double]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "results_svm = fit_model.transform(test_data)\n",
        "display(results_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+\n",
            "|ProdTaken|prediction|\n",
            "+---------+----------+\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        1|       1.0|\n",
            "|        0|       1.0|\n",
            "|        0|       1.0|\n",
            "+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results_svm.select(['ProdTaken','prediction']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy of the decision tree classifier is 0.688360450563204\n"
          ]
        }
      ],
      "source": [
        "ACC_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"ProdTaken\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "accuracy = ACC_evaluator.evaluate(results_svm)\n",
        "\n",
        "print(f\"The accuracy of the decision tree classifier is {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The area under the curve is 0.6888870703764322\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='ProdTaken',metricName='areaUnderROC')\n",
        "\n",
        "AUC = AUC_evaluator.evaluate(results_svm)\n",
        "print(\"The area under the curve is {}\".format(AUC))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The area under the PR curve is 0.645005858599166\n"
          ]
        }
      ],
      "source": [
        "PR_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='ProdTaken',metricName='areaUnderPR')\n",
        "PR = PR_evaluator.evaluate(results_svm)\n",
        "print(\"The area under the PR curve is {}\".format(PR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is the confusion matrix \n",
            " [[809 413]\n",
            " [334 841]]\n"
          ]
        }
      ],
      "source": [
        "y_true_svm = results_svm.select(\"ProdTaken\")\n",
        "y_true_svm = y_true_svm.toPandas()\n",
        " \n",
        "y_pred_svm = results_svm.select(\"prediction\")\n",
        "y_pred_svm = y_pred_svm.toPandas()\n",
        " \n",
        "cnf_matrix_svm = confusion_matrix(y_true_svm, y_pred_svm)\n",
        "\n",
        "print(\"Below is the confusion matrix \\n {}\".format(cnf_matrix_svm))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.68\n",
            "Precision: 0.67\n",
            "Recall: 0.69\n",
            "F1 Score: 0.68\n"
          ]
        }
      ],
      "source": [
        "tn = cnf_matrix_svm[0][0]\n",
        "fp = cnf_matrix_svm[0][1]\n",
        "fn = cnf_matrix_svm[1][0]\n",
        "tp = cnf_matrix_svm[1][1]\n",
        "\n",
        "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "f1_score = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1_score:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model-4 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Random Forest Classifier and its hyperparameters for tuning\n",
        "rf = RandomForestClassifier(\n",
        "    labelCol=\"ProdTaken\",\n",
        "    featuresCol=\"features\",\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
        "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
        "    .build()\n",
        "\n",
        "# Define evaluator for binary classification and multiclass classification\n",
        "evaluator_auc = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"ProdTaken\", metricName=\"areaUnderROC\")\n",
        "evaluator_pr = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"ProdTaken\", metricName=\"areaUnderPR\")\n",
        "evaluator_acc = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"ProdTaken\", metricName=\"accuracy\")\n",
        "\n",
        "# Define cross-validator for hyperparameter tuning\n",
        "cv = CrossValidator(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator_auc,\n",
        "    numFolds=5,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_rf= Pipeline(stages=[\n",
        "    TypeofContact_indexer,\n",
        "    Occupation_indexer,\n",
        "    Gender_indexer,\n",
        "    ProductPitched_indexer,\n",
        "    MaritalStatus_indexer,\n",
        "    Designation_indexer,\n",
        "    encoder,\n",
        "    assembler,\n",
        "    cv\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit pipeline to training data\n",
        "model_rf = pipe_rf.fit(train_data)\n",
        "\n",
        "# Predict on test data\n",
        "predictions_rf = model_rf.transform(test_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy of the Random Forest classifier is 0.950354609929078\n"
          ]
        }
      ],
      "source": [
        "accuracy = evaluator_acc.evaluate(predictions_rf)\n",
        "\n",
        "print(f\"The accuracy of the Random Forest classifier is {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The area under the curve is 0.950949263502455\n"
          ]
        }
      ],
      "source": [
        "# Print evaluation metrics of the model\n",
        "auc = evaluator_auc.evaluate(predictions_rf)\n",
        "\n",
        "\n",
        "print(\"The area under the curve is {}\".format(auc))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The area under the PR curve is 0.9183538356278682\n"
          ]
        }
      ],
      "source": [
        "pr = evaluator_pr.evaluate(predictions_rf)\n",
        "\n",
        "print(\"The area under the PR curve is {}\".format(pr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is the confusion matrix \n",
            " [[1125   97]\n",
            " [  22 1153]]\n"
          ]
        }
      ],
      "source": [
        "y_true_rf = predictions_rf.select(\"ProdTaken\")\n",
        "y_true_rf = y_true_rf.toPandas()\n",
        " \n",
        "y_pred_rf = predictions_rf.select(\"prediction\")\n",
        "y_pred_rf = y_pred_rf.toPandas()\n",
        " \n",
        "cnf_matrix_rf = confusion_matrix(y_true_rf, y_pred_rf)\n",
        "\n",
        "print(\"Below is the confusion matrix \\n {}\".format(cnf_matrix_rf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.95\n",
            "Precision: 0.92\n",
            "Recall: 0.98\n",
            "F1 Score: 0.95\n"
          ]
        }
      ],
      "source": [
        "tn = cnf_matrix_rf[0][0]\n",
        "fp = cnf_matrix_rf[0][1]\n",
        "fn = cnf_matrix_rf[1][0]\n",
        "tp = cnf_matrix_rf[1][1]\n",
        "\n",
        "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "f1_score = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1_score:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion:\n",
        "\n",
        "\n",
        "The following four models were trained for the prediction task-\n",
        "\n",
        "1. Logistic Regression\n",
        "2. Decision Tree\n",
        "3. Support Vector Machines\n",
        "4. Random Forest\n",
        "\n",
        "Out of these, the ensemble model- \"Random Forest with hyperparameter tuning\" performed the best with a recall score of 98%.\n",
        "\n",
        "So, this model can be used by the company to predict the potential purchasers of their \"new package\" based on the available customer data and subsequently tailor their marketing campaign to reduce the marketing costs and/or improve customer retention. \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "spark",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "58de57e6548b5de3461be3d3eb54bdd354074e6dbd2cdf12de850d71e06dd30b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
